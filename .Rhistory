aes(x = CAGR,y = value,fill = variable)) + geom_bar(stat = "identity")
melted.pareto.minor = melt(pareto.minor,id = c("CAGR","DD","SD"))
melted.pareto.minor = melt(pareto.minor,id = c("CAGR","DD"))
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) + geom_bar(stat = "identity")
plot.stacked
max(df.results.minor$CAGR)
min(df.results.minor$CAGR)
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity") +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR)))
plot.stacked
melted.pareto.minor
plot
df.results.minor
pareto.minor
a = seq(from = 0, to = 1,by = .05)
b = expand.grid(a,a,a,a)
c = b[rowSums(b) == 1,]
c
nrow(c)
source('D:/Dropbox/Github/retirement-planner/Harry Browne/harry_browne_shortterm.R')
plot
plot.stacked
df.pareto.minor
pareto.minor
plot
plot.stacked
plot
plot.stacked
plot.stacked
plot
plot.stacked
plot = ggplot() +
geom_point(data = df.results.minor,aes(x = CAGR,y = DD),colour = "gray") +
geom_point(data = pareto.minor,aes(x = CAGR,y = DD),colour = "orange")
plot
plot = ggplot() +
geom_point(data = df.results.minor,aes(x = CAGR,y = DD),colour = "gray") +
geom_point(data = pareto.minor,aes(x = CAGR,y = DD),colour = "orange")
plot.stacked
library(gridExtra)
grid.arrange(plot,plot.stacked,ncol = 1)
nrow(melted.pareto.minor)
nrow(pareto.minor)
plot.stacked
melted.pareto.minor
plot.stacked
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.5) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR)))
plot.stacked
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.01) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR)))
plot.stacked
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.001) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR)))
plot.stacked
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.01) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR)))
plot.stacked
grid.arrange(plot,plot.stacked,ncol = 1)
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.01) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR))) +
theme(legend.position="bottom")
plot.stacked
grid.arrange(plot,plot.stacked,ncol = 1)
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.0001) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR))) +
theme(legend.position="bottom")
plot.stacked
grid.arrange(plot,plot.stacked,ncol = 1)
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.005) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR))) +
theme(legend.position="bottom")
grid.arrange(plot,plot.stacked,ncol = 1)
c
save(data = df.results.minor,file = "df.harrybrowne.rda")
save(data = df.results.minor,file = "df.10_harrybrowne_spectrum.rda")
pareto.minor
load("df.10_harrybrowne_spectrum.rda")
pareto.minor = psel(df.results.minor, high(CAGR) * low(DD))
melted.pareto.minor = melt(pareto.minor,id = c("CAGR","DD"))
plot = ggplot() +
geom_point(data = df.results.minor,aes(x = CAGR,y = DD),colour = "gray") +
geom_point(data = pareto.minor,aes(x = CAGR,y = DD),colour = "orange")
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.005) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR))) +
theme(legend.position="bottom")
plot
plot.stacked
grid.arrange(plot,plot.stacked,ncol = 1)
library(ggplot2)  # for ggplot
library(rPref)    # for psel
library(reshape2) # for melt
library(gridExtra)
load("df.10_harrybrowne_spectrum.rda")
pareto.minor = psel(df.results.minor, high(CAGR) * low(DD))
melted.pareto.minor = melt(pareto.minor,id = c("CAGR","DD"))
plot = ggplot() +
geom_point(data = df.results.minor,aes(x = CAGR,y = DD),colour = "gray") +
geom_point(data = pareto.minor,aes(x = CAGR,y = DD),colour = "orange")
plot.stacked = ggplot(data = melted.pareto.minor,
aes(x = CAGR,y = value,fill = variable)) +
geom_bar(stat = "identity",width=.005) +
scale_x_continuous(limits = c(min(df.results.minor$CAGR),  max(df.results.minor$CAGR))) +
theme(legend.position="bottom")
grid.arrange(plot,plot.stacked,ncol = 1)
df.results.minor
save(data = df.results.minor,file = "df.10_harrybrowne_spectrum.rda")
getwd()
setwd("D:/Github/retirement-planner/Harry Browne")
getwd()
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
doc
doc.html
doc.html <- htmlTreeParse(url[1],useInternal = TRUE)
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
df
url
htmlTreeParse(url[1],useInternal = TRUE)
library(RCurl)
doc.html <- htmlTreeParse(url[1],useInternal = TRUE)
doc.html <- htmlTreeParse(url[1])
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
doc.html
doc.text = unlist(xpathApply(doc.html, '//p', xmlValue))
doc.text
doc.html
clear
doc.html
hello
doc.html
url[1]
doc.html <- htmlTreeParse(url[1],useInternal = TRUE)
doc.html
install.packages("rvest")
library(rvest)
site = html(url[1])
site = read_html(url[1])
site
class(site)
str(site)
xpath <- "//*[contains(concat( " ", @class, " " ), concat( " ", "pull-right", " " ))]"
xpath
xpath <- "//*[contains(concat( ' ', @class, ' ' ), concat( ' ', 'pull-right', ' ' ))]"
xpath
url <- paste("http://etfdb.com/etf/", df$Symbol, sep="")
tree <- htmlTreeParse(url, useInternalNodes=TRUE)
xpath <- "//*[contains(concat( ' ', @class, ' ' ), concat( ' ', 'pull-right', ' ' ))]"
xpath
tree
node <- getNodeSet(tree, xpath)
node
node
operation <- tryCatch(readHTMLTable(node[[2]]), error = function(e) NA)
overview <- tryCatch(readHTMLTable(node[[1]]), error = function(e) NA)
operation
overview
site %>%
html_node("pull right") %>%
html_text() %>%
as.numeric()
site %>%
html_node("pull-right") %>%
html_text() %>%
as.numeric()
site
site[1]
site[2]
site[[1]]
site
lego_movie <- html("http://www.imdb.com/title/tt1490017/")
lego_movie <- read_html("http://www.imdb.com/title/tt1490017/")
lego_movie
lego_movie %>%
html_node("strong span") %>%
html_text() %>%
as.numeric()
li~ li+ li .pull-rightsite %>%
html_node("li~ li+ li .pull-right") %>%
html_text() %>%
as.numeric()
site %>%
html_node("li~ li+ li .pull-right") %>%
html_text() %>%
as.numeric()
site %>%
html_node("li~ li+ li .pull-right") %>%
html_text() %>%
as.numeric()site %>%
html_node("pull-right") %>%
html_text() %>%
as.numeric()
site %>%
html_node("pull-right") %>%
html_text() %>%
as.numeric()
site %>%
html_node("li") %>%
html_text() %>%
as.numeric()
site
site %>%
html_node("Expense Ratio:") %>%
html_text() %>%
as.numeric()
site %>%
html_node("Expense") %>%
html_text() %>%
as.numeric()
site %>%
html_node("Expense") %>%
html_text() %>%
as.numeric()
html_nodes(doc, xpath = "//table//td")
tmlpage <- html("http://forecast.weather.gov/MapClick.php?lat=42.31674913306716&lon=-71.42487878862437&site=all&smap=1#.VRsEpZPF84I")
forecasthtml <- html_nodes(htmlpage, "#detailed-forecast-body b , .forecast-text")
forecast <- html_text(forecasthtml)
paste(forecast, collapse =" ")
vignette("selectorgadget")
cast = html_nodes(lego_movie, ".itemprop")
cast
cast = html_nodes(lego_movie, "titleCast .itemprop")
cast
html = read_html("http://www.imdb.com/title/tt1490017/")
cast = html_nodes(html, "titleCast .itemprop")
cast
html_text(cast)
cast <- html_nodes(html, "#titleCast span.itemprop")
cast
html_text(cast)
cast <- html_nodes(site, ".pull-right")
cast
html_text(cast)
site
site = htmlTreeParse(url[1])
cast <- html_nodes(site, ".pull-right")
html_text(cast)
cast
html_text
html_text(cast)
site = htmlTreeParse(url[1])
cast <- html_nodes(site, "li~ li+ li .pull-right")
html_text(cast)
cast
html_text(cast)
html_text(cast)[1]
html_text(cast)[2]
html_text(cast)[3]
html_text(cast)[4]
html_text(cast)[5]
html_text(cast)[6]
html_text(cast)[7]
html_text(cast)[8]
html_text(cast)[9]
df
head(df)
# 9/24/2016
# ETF Metadata Scraper
# This program scrapes ETF metadata according to Paul Lam's article
# https://www.quantisan.com/tag/ETF/
rm(list=ls())
directory = "D:/Github/DataDrivenTrading"
setwd(directory)
# Load libraries
library(XML)
library(RCurl)
# Load list of ETFs
# Source: http://www.nasdaq.com/etfs/list
# Only first two columns have relevant information
df = read.csv(file="ETFList.csv", sep=",")[,1:2]
# Make necessary columns
df["Issuer"] = NULL
df["ER"] = NULL
df["Inception"] = NULL
df["Benchmark"] = NULL
df
head(df)
df
df$Issuer = NULL
df
head(df)
df$size  = 0
df
df$size  = NULL
df
df$size  = NA
df
# Make necessary columns
df["Issuer"] = NA
df["ER"] = NA
df["Inception"] = NA
df["Benchmark"] = NA
df
head(df)
url
url <- paste("http://etfdb.com/etf/", df$Symbol, sep="")
url
class(url)
url[1]
length(url)
nrow(df)
html_text(cast)
cast
site = htmlTreeParse(url[i])
cast <- html_nodes(site, ".pull-right")
html_text(cast)
site = htmlTreeParse(url[1])
cast = html_nodes(site,".pull-right")
cast = html_nodes(site,".pull-right")
html_text(cast)[7]
cast <- html_nodes(site, ".pull-right")
site
url
url[1]
site
site = htmlTreeParse(url[1])
cast <- html_nodes(site, "li~ li+ li .pull-right")
html_text(cast)
site
site = htmlTreeParse(url[1])
cast <- html_nodes(site, "li~ li+ li .pull-right")
html_text(cast)
cast <- html_nodes(site, "li~ li+ li .pull-right")
site
site = htmlTreeParse(url[1])
cast <- html_nodes(site, "li~ li+ li .pull-right")
html_text(cast)
?html_nodes
site
url
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
url
df
url
site = htmlTreeParse(url[1])
cast <- html_nodes(site, "li~ li+ li .pull-right")
cast <- html_nodes(site, ".pull-right")
cast - html_nodes(site, ".pull-right")
cast = html_nodes(site, ".pull-right")
cast = html_nodes(site,".pull-right")
cast = html_nodes(doc,".pull-right")
cast = html_nodes(site,".pull-right")
site = htmlParse(url[1])
site
cast <- html_nodes(site, "li~ li+ li .pull-right")
site = htmlTreeParse(url[1])
cast <- html_nodes(site, ".pull-right")
site = read_html(url[1])
site
cast <- html_nodes(site, ".pull-right")
cast
html_text(cast)
html_text(cast)[7]
html_text(cast)[8]
html_text(cast)[1]
html_text(cast)[2]
html_text(cast)[3]
html_text(cast)[4]
html_text(cast)[45]
html_text(cast)[5]
html_text(cast)[6]
html_text(cast)[5]
html_text(cast)[6]
html_text(cast)[7]
html_text(cast)[8]
html_text(cast)[9]
html_text(cast)[1-]
html_text(cast)[10]
html_text(cast)[11]
html_text(cast)[12]
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
df
head(df,10)
head(df,10)
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
df
head(df,100)
View(df)
save(df,file = "etf_data.rda")
for (i in 1:nrow(df)){
site = read_html(url[i])
}
list[8]
list[[8]]
all.sites = vector("list",3)
all.sites
all.sites = vector("list",nrow(df))
for (i in 1:nrow(df)){
all.sites[[i]] = read_html(url[i])
}
site
all.sites
all.sites[[2]]
all.sites[[50]]
all.sites[[100]]
all.sites[[200]]
all.sites[[164]]
all.sites[[150]]
all.sites[[156]]
all.sites[[163]]
all.sites[[164]]
all.sites[[163]]
all.sites[[164]]
pause(0.5)
Sys.sleep(0.1)
Sys.sleep(0.2)
source('D:/Github/DataDrivenTrading/etf_metadata_scraper.R')
}all.sites
all.sites
all.sites
all.sites[[100]]
all.sites[[500]]
all.sites[[600]]
all.sites[[60]]
all.sites[[10]]
all.sites[[11]]
all.sites[[12]]
all.sites[[13]]
all.sites[[25]]
all.sites[[2-]]
all.sites[[20]]
all.sites[[21]]
all.sites[[22]
all.sites[[22]]
all.sites[[21]]
all.sites[[22]]
all.sites[[24]]
all.sites[[23]]
all.sites[[22]]
Sys.sleep(0.2)
Sys.sleep(0.3)
Sys.sleep(0.1)
# Save all web content first
all.sites = vector("list",nrow(df))
for (i in 1:nrow(df)){
all.sites[[i]] = read_html(url[i])
Sys.sleep(0.1)
}
all.sites[[50]]
all.sites[[100]]
all.sites[[163]]
all.sites[[164]]
all.sites[[163]]
all.sites[[163]]
all.sites[[164]]
all.sites[[163]]
for (i in 1:163){
cast <- html_nodes(site[[i]], ".pull-right")
df$Issuer[i] = html_text(cast)[6]
df$Structure[i] = html_text(cast)[7]
df$Expense.Ratio[i] = html_text(cast)[8]
df$Inception[i] = html_text(cast)[10]
df$Tax.Form[i] = html_text(cast)[11]
df$Tracking.Index[i] = html_text(cast)[12]
}
for (i in 1:163){
cast <- html_nodes(all.sites[[i]], ".pull-right")
df$Issuer[i] = html_text(cast)[6]
df$Structure[i] = html_text(cast)[7]
df$Expense.Ratio[i] = html_text(cast)[8]
df$Inception[i] = html_text(cast)[10]
df$Tax.Form[i] = html_text(cast)[11]
df$Tracking.Index[i] = html_text(cast)[12]
}
df
head(df)
View(df)
df$Symbol
df$Name
gsub( " .*$", "", df$Name )
df["Issuer"] = gsub( " .*$", "", df$Name )
df
head(df)
# 9/24/2016
# ETF Metadata Scraper
# This program scrapes ETF metadata from ETFDB
rm(list=ls())
directory = "D:/Github/DataDrivenTrading"
setwd(directory)
# Load libraries
library(XML)
library(RCurl)
# Load list of ETFs
# Source: http://www.nasdaq.com/etfs/list
# Only first two columns have relevant information
df = read.csv(file="ETFList.csv", sep=",")[,1:2]
# Create requisitie columns
df["Issuer"] = gsub( " .*$", "", df$Name )
df["Structure"] = NA
head9df
head(df)
unique(df$Issuer)
